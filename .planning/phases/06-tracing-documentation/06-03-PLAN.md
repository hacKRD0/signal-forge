---
phase: 06-tracing-documentation
type: execute
---

<objective>
Final testing, polish, and project verification.

Purpose: Ensure the complete prototype works end-to-end, fix any issues, and verify all success criteria from the brief are met.
Output: Fully functional, tested, documented prototype ready for delivery.
</objective>

<execution_context>
@~/.claude/skills/create-plans/workflows/execute-phase.md
@~/.claude/skills/create-plans/templates/summary.md
@~/.claude/skills/create-plans/references/checkpoints.md
</execution_context>

<context>
@.planning/BRIEF.md
@.planning/ROADMAP.md
@README.md
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end integration tests</name>
  <files>tests/test_end_to_end.py, tests/conftest.py, tests/fixtures/</files>
  <action>
Create tests/fixtures/ directory:
- Add sample documents (small test files):
  - sample.docx (1-page business doc)
  - sample.pdf (1-page business doc)
  - sample.csv (business data table)
  - sample.pptx (3-slide presentation)
- Documents describe a fictional SaaS company

Create tests/conftest.py:
- Pytest fixtures for test data
- Mock fixtures for API calls (if testing without real API key)

Create tests/test_end_to_end.py:
- Test: test_document_parsing_all_formats
  - Parse each fixture file
  - Verify text extracted successfully
  - No errors
- Test: test_context_extraction_mock
  - Mock agent response
  - Extract context from sample docs
  - Verify BusinessContext populated
- Test: test_full_customer_discovery_mock
  - Mock all API calls
  - Run full customer discovery pipeline
  - Verify DiscoveryResult with 10 results
  - Verify scores and rationales attached
- Test: test_full_partner_discovery_mock
  - Same as customer but for partners
- Test: test_tracing_enabled
  - Run discovery with tracing
  - Verify traces.jsonl and run_reports.jsonl created
  - Verify content structure
- Mark integration tests requiring API key with @pytest.mark.integration

Avoid: Don't make tests depend on external APIs by default (use mocks). Integration tests optional.
  </action>
  <verify>
- tests/fixtures/ contains sample documents
- tests/test_end_to_end.py exists
- Tests run: python -m pytest tests/test_end_to_end.py -v
- All mocked tests pass
- Integration tests marked appropriately
  </verify>
  <done>End-to-end integration tests created and passing</done>
</task>

<task type="auto">
  <name>Task 2: Verify all success criteria from BRIEF</name>
  <files>VERIFICATION.md</files>
  <action>
Create VERIFICATION.md checklist:

Review each success criterion from .planning/BRIEF.md:
- [ ] Discovers 10 potential customers + 10 potential partners
- [ ] Accepts DOCX, PDF, CSV, PPTX files
- [ ] Provides results: company name, website, locations, size, rationale, sources
- [ ] UX allows natural language queries with optional filters
- [ ] All prompts and outputs traced to local JSONL with timestamps
- [ ] Complete README with setup/run instructions
- [ ] requirements.txt present
- [ ] Runnable locally without external API keys (beyond Google ADK)

For each criterion:
- Test manually or via automated test
- Document verification method
- Mark as PASS or FAIL
- If FAIL: document issue and create fix task

Write verification results in VERIFICATION.md with status and notes

Avoid: Don't skip any criteria. Verify thoroughly.
  </action>
  <verify>
- VERIFICATION.md exists
- All success criteria checked
- Status documented (PASS/FAIL)
- Any failures have fix tasks identified
  </verify>
  <done>Success criteria verified and documented</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete prototype with all features, documentation, and tests</what-built>
  <how-to-verify>
1. Fresh install test:
   - Create new directory
   - Copy/clone project
   - Follow README.md installation instructions exactly
   - Verify setup works without issues

2. Full workflow test:
   - Set up .env with Google API key
   - Run: streamlit run app.py
   - Upload test documents (from tests/fixtures/ or create new ones)
   - Process documents → verify context extraction
   - Run customer discovery with filters
   - Verify 10 customer results with scores, rationales, sources
   - Run partner discovery with different filters
   - Verify 10 partner results with scores, rationales, sources
   - Download CSV and JSON exports
   - Verify downloads contain expected data

3. Tracing verification:
   - Check logs/ directory
   - Open logs/traces.jsonl
   - Verify all prompts and responses logged
   - Open logs/run_reports.jsonl
   - Verify run reports include timestamps, sources, trace IDs
   - Verify JSONL format valid (use jq or JSON viewer)

4. Documentation check:
   - Read README.md
   - Verify all sections clear and accurate
   - Follow troubleshooting steps
   - Read ARCHITECTURE.md
   - Check API_KEY_SETUP.md guide

5. Requirements check:
   - Verify requirements.txt installs cleanly
   - No missing dependencies
   - No version conflicts

6. Success criteria review:
   - Review VERIFICATION.md
   - Confirm all criteria PASS
   - Note any issues or improvements

7. Code quality check:
   - No obvious bugs or errors
   - Code organized and documented
   - Logging appropriate (not too verbose)

Confirm prototype meets all brief requirements and is ready for delivery
  </how-to-verify>
  <resume-signal>Type "approved" if prototype complete and ready, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] End-to-end tests created and passing
- [ ] Success criteria verification documented
- [ ] Human verification completed successfully
- [ ] Any identified issues fixed
- [ ] Prototype fully functional and documented
- [ ] Phase 6 complete - project ready for delivery
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- All success criteria from BRIEF met and verified
- Human verification confirms quality and completeness
- Phase 6 complete - prototype ready for delivery
- ALL 6 phases complete - project delivered successfully
</success_criteria>

<output>
After completion, create `.planning/phases/06-tracing-documentation/06-03-SUMMARY.md`:

# Phase 6 Plan 3: Final Testing & Verification Summary

**Project complete and verified**

## Accomplishments
- Created end-to-end integration tests
- Verified all success criteria from BRIEF
- Human verification completed
- Any issues identified and fixed
- Complete prototype delivered
- Phase 6 complete
- **ALL 6 PHASES COMPLETE - PROJECT DELIVERED**

## Files Created/Modified
- `tests/test_end_to_end.py` - E2E integration tests
- `tests/conftest.py` - Test fixtures
- `tests/fixtures/` - Sample test documents
- `VERIFICATION.md` - Success criteria verification
- [Any fixes made during verification]

## Decisions Made
- Mock-based tests for CI (integration tests optional)
- Sample fixtures for testing all formats
- Manual verification for UI/UX quality
- All success criteria verified before completion

## Issues Encountered
[Document any issues found and how they were fixed]

## Final Status
✅ **PROJECT COMPLETE**
- All 6 phases delivered
- All 18 plans executed successfully
- All success criteria met
- Prototype fully functional
- Documentation complete
- Ready for delivery

## Deliverables
1. ✅ Complete runnable codebase
2. ✅ README.md with setup + run instructions
3. ✅ requirements.txt with all dependencies
4. ✅ Modular, documented Python files
5. ✅ Streamlit UX with file upload, queries, results table
6. ✅ Customer and partner discovery (10 each)
7. ✅ Results with: name, website, locations, size, rationale, sources
8. ✅ Prompt tracing to JSONL files
9. ✅ Run reports with timestamps and sources

## Next Step
Project complete. Prototype ready for user evaluation and feedback.
</output>
