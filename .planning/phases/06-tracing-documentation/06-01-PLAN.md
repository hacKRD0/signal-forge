---
phase: 06-tracing-documentation
type: execute
---

<objective>
Implement prompt tracing and evaluation artifacts system.

Purpose: Capture all prompts and agent outputs to JSONL files with timestamps for evaluation, debugging, and transparency.
Output: Tracing system that logs all agent interactions to local JSONL files with run reports.
</objective>

<execution_context>
@~/.claude/skills/create-plans/workflows/execute-phase.md
@~/.claude/skills/create-plans/templates/summary.md
</execution_context>

<context>
@.planning/BRIEF.md
@.planning/ROADMAP.md
@src/agent/discovery_agent.py
@src/scoring/rationale_generator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tracing data models and JSONL writer</name>
  <files>src/tracing/__init__.py, src/tracing/models.py, src/tracing/jsonl_writer.py</files>
  <action>
Create src/tracing/__init__.py (package)

Create src/tracing/models.py:
- Dataclass: PromptTrace with fields:
  - trace_id: str (UUID)
  - timestamp: datetime
  - prompt_type: str (context_extraction, customer_discovery, partner_discovery, scoring, rationale)
  - prompt_text: str
  - response_text: str
  - metadata: dict (optional context info)
- Dataclass: RunReport with fields:
  - run_id: str (UUID)
  - start_time: datetime
  - end_time: datetime
  - entity_type: str (customer/partner)
  - num_results: int
  - traces: list[str] (trace IDs)
  - sources: list[str] (URLs used)
  - status: str (success/error)
  - error_message: str = None
- Methods: to_dict() for both dataclasses

Create src/tracing/jsonl_writer.py:
- Function: write_trace(trace: PromptTrace, filepath: str)
  - Convert trace to dict
  - Append as JSON line to file
  - Create file/directory if doesn't exist
- Function: write_run_report(report: RunReport, filepath: str)
  - Write run report as JSON line
  - Separate file from traces

Avoid: Don't use databases (JSONL is simple, portable). Don't lose any traces (append mode).
  </action>
  <verify>
- src/tracing/models.py exists
- src/tracing/jsonl_writer.py exists
- Imports work: python -c "from src.tracing.models import *"
- Dataclasses have all required fields
  </verify>
  <done>Tracing data models and JSONL writer created</done>
</task>

<task type="auto">
  <name>Task 2: Integrate tracing into agent and scoring components</name>
  <files>src/agent/discovery_agent.py, src/scoring/rationale_generator.py, src/tracing/tracer.py</files>
  <action>
Create src/tracing/tracer.py:
- Class: Tracer
- __init__: Accept output directory path (default: logs/)
- Method: trace_prompt(prompt_type: str, prompt_text: str, response_text: str, metadata: dict = None) -> str
  - Create PromptTrace instance
  - Generate trace_id (UUID)
  - Write to JSONL file: {output_dir}/traces.jsonl
  - Return trace_id
- Method: start_run(entity_type: str) -> str
  - Create run_id
  - Store start_time
  - Return run_id
- Method: end_run(run_id: str, num_results: int, traces: list[str], sources: list[str], status: str, error: str = None)
  - Create RunReport
  - Write to JSONL file: {output_dir}/run_reports.jsonl
- Singleton pattern or class instance passed to components

Update src/agent/discovery_agent.py:
- Add tracer parameter to __init__
- In each method (extract_context, find_customers, find_partners):
  - After agent call: call tracer.trace_prompt()
  - Include prompt type, full prompt, response

Update src/scoring/rationale_generator.py:
- Add tracer parameter
- Trace rationale generation prompts and responses

Avoid: Don't make tracing slow down operations (async writes if needed). Don't fail if tracing fails (log error, continue).
  </action>
  <verify>
- src/tracing/tracer.py exists
- DiscoveryAgent and RationaleGenerator updated with tracing
- Traces written to logs/traces.jsonl
- Run reports written to logs/run_reports.jsonl
- No errors when tracing enabled
  </verify>
  <done>Tracing integrated into agent and scoring components</done>
</task>

<task type="auto">
  <name>Task 3: Add run report generation and source collection</name>
  <files>src/ui/discovery_runner.py, src/tracing/source_collector.py</files>
  <action>
Create src/tracing/source_collector.py:
- Function: collect_sources(results: DiscoveryResult) -> list[str]
  - Extract all source URLs from all companies
  - Deduplicate
  - Return sorted list

Update src/ui/discovery_runner.py:
- Import Tracer, collect_sources
- Create Tracer instance
- In run_discovery():
  - Call tracer.start_run() at beginning
  - Pass tracer to all components
  - Collect all trace_ids during discovery
  - After discovery: collect sources
  - Call tracer.end_run() with results
  - Handle errors: mark status as "error" with error message

Ensure all traces and sources captured for complete run report

Avoid: Don't lose trace IDs (collect them as discovery proceeds). Don't skip run report on errors (still write report).
  </action>
  <verify>
- source_collector.py exists
- discovery_runner.py generates run reports
- logs/run_reports.jsonl contains complete run information
- Sources collected from all results
- Error runs also logged
  </verify>
  <done>Run report generation and source collection implemented</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Tracing models (PromptTrace, RunReport) defined
- [ ] JSONL writer creates trace and report files
- [ ] Agent and scoring components trace all prompts
- [ ] Run reports include timestamps, traces, and sources
- [ ] Tracing doesn't break on errors
- [ ] logs/ directory contains traces.jsonl and run_reports.jsonl
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- All prompts and responses logged to JSONL
- Run reports provide complete execution overview
- Sources collected and logged
- Ready for README and final documentation
</success_criteria>

<output>
After completion, create `.planning/phases/06-tracing-documentation/06-01-SUMMARY.md`:

# Phase 6 Plan 1: Prompt Tracing Summary

**Prompt tracing and evaluation artifacts system**

## Accomplishments
- Created tracing data models (PromptTrace, RunReport)
- Implemented JSONL writer for trace storage
- Integrated tracing into discovery agent and scoring components
- Run report generation with timestamps and sources
- Source collection from discovery results
- All prompts and outputs logged to logs/ directory

## Files Created/Modified
- `src/tracing/__init__.py` - Package initialization
- `src/tracing/models.py` - PromptTrace, RunReport models
- `src/tracing/jsonl_writer.py` - JSONL file writing
- `src/tracing/tracer.py` - Tracer class
- `src/tracing/source_collector.py` - Source extraction
- `src/agent/discovery_agent.py` - Added tracing
- `src/scoring/rationale_generator.py` - Added tracing
- `src/ui/discovery_runner.py` - Run report generation

## Decisions Made
- JSONL format (simple, append-friendly, tool-compatible)
- Separate files for traces and run reports
- UUID trace IDs for uniqueness
- Source deduplication in reports
- Continue on tracing errors (don't break discovery)

## Issues Encountered
[Document any issues, or "None"]

## Next Step
Ready for 06-02-PLAN.md (README and requirements.txt)
</output>
